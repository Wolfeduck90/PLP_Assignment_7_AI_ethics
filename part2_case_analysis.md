# Part 2: Case Study Analysis (40%)

---

## âš™ï¸ Case Study 1: Biased Hiring Tool (Amazon)

### ğŸ¯ Identifying the Source of Bias
- Training data favored male applicants due to historical hiring trends.
- Gendered language and feature selection penalized female-associated terms.
- No fairness constraints in the model design.

### âœ… Proposed Fixes
1. Rebalance the training data with diverse candidate profiles.
2. Introduce fairness-aware algorithms to reduce bias during model training.
3. Audit feature importance and remove indirect gender indicators.

### ğŸ“Š Fairness Evaluation Metrics
- **Demographic Parity** â€“ Equal selection rates across gender groups.
- **Equal Opportunity** â€“ Comparison of true positive rates between male and female candidates.
- **Disparate Impact Ratio** â€“ Should fall between 0.8 and 1.25 to be considered fair.

---

## ğŸš“ Case Study 2: Facial Recognition in Policing

### âš ï¸ Ethical Risks
- Wrongful arrests from misidentification.
- Invasion of privacy via mass surveillance.
- Reinforcement of societal bias against minority groups.
- Erosion of public trust in law enforcement and AI.

### ğŸ“œ Recommended Deployment Policies
1. **Bias Audits:** Conduct third-party evaluations regularly.
2. **Human Oversight:** Require manual verification before action.
3. **Transparency Reports:** Disclose model performance and errors.
4. **Public Consultation:** Engage communities before system deployment.
5. **Restricted Usage:** Limit application to cases with high societal benefit, like finding missing persons.

---
